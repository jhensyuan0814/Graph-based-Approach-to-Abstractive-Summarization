{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import math\n",
    "from operator import eq \n",
    "\n",
    "def OpinosisGraph(Z, G, PRI):  #paper Alg1\n",
    "    for i in range(len(Z)):  \n",
    "        for j in range(len(Z[i])):  \n",
    "            LABEL = Z[i][j]\n",
    "            PID = j\n",
    "            SID = i\n",
    "            if LABEL in G:\n",
    "                PRI[LABEL].append((SID, PID))\n",
    "            else:\n",
    "                G[LABEL] = list()\n",
    "                PRI[LABEL] = [(SID, PID)]\n",
    "            if j > 0 and LABEL not in G[Z[i][j-1]]:\n",
    "                G[Z[i][j-1]].append(Z[i][j])\n",
    "                \n",
    "def VSN(node):   #paper def 1\n",
    "    global PRI1\n",
    "    sigma_vsn = 15\n",
    "    ele = PRI1[node]\n",
    "    sum = 0\n",
    "    for i in range(len(ele)):\n",
    "        sum = sum + ele[i][1]\n",
    "    return (sum/len(ele)) <= sigma_vsn\n",
    "\n",
    "def VEN(node):   #paper def 2\n",
    "    prohibit = ['and', 'but', 'for', 'nor', 'or', 'so', 'yet', ',', '.', '!', '?']\n",
    "    if(node[0] in prohibit):\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "def collapsible(node):  #paper def 5\n",
    "    if(node[1] == 'VERB'):\n",
    "        return True\n",
    "    else: \n",
    "        return False\n",
    "\n",
    "def valid_sent(sent):     #paper doesn't specify how to find a valid sentence in grammar\n",
    "    prohibit = {'VERB':['VERB'],      #We check the part of speech of two adjacent words and prevent two same words appearing in one sentence\n",
    "        'NOUN':['ADJ','PRON','NOUN'],\n",
    "        'ADJ':['VERB','PRON','ADV'],\n",
    "        'ADV':['ADJ','NOUN','PRON','ADV'],\n",
    "        'PRON':['ADJ','PRON','NOUN'], #I,she\n",
    "        'CONJ':['CONJ','PRT','ADP'], #and\n",
    "        'PRT':['CONJ','PRT','ADP'], # 's\n",
    "        '.':['VERB','.','ADJ','PRON','NOUN','ADV','PRON','CONJ','PRT','ADP','NUM','DET','X'], #punctuation\n",
    "        'ADP':['CONJ','PRT','ADP'], #prep+介詞\n",
    "        'NUM':[],\n",
    "        'DET':['CONJ','PRT','ADP'], #to\n",
    "        'X':[]\n",
    "      }\n",
    "    for i in range(0,len(sent)-1):\n",
    "        if(sent[i+1][1] in prohibit[sent[i][1]]):\n",
    "              return False\n",
    "    buf = []\n",
    "    for i in sent:\n",
    "        if(i not in buf):\n",
    "            buf.append(i)\n",
    "        elif(i[1] == 'ADJ' or i[1] == 'ADV' or i[1] == 'NOUN'):\n",
    "            return False\n",
    "    return True\n",
    "\n",
    "def check_inter(PRIa,PRIb):  #paper Alg3 line12\n",
    "    PRIc = []\n",
    "    sigma_gap = 4\n",
    "    for first in PRIa:\n",
    "        for second in PRIb:\n",
    "            if(first[0] == second[0] and abs(first[1]-second[1]) <= sigma_gap):\n",
    "                PRIc.append(second)\n",
    "    return PRIc\n",
    "\n",
    "def eliminate_dup(clist): #paper Alg3 line22\n",
    "    buf = []\n",
    "    for ele in clist:\n",
    "        if ele[0] not in buf:\n",
    "            buf.append(ele[0])\n",
    "        else:\n",
    "            clist.remove(ele)\n",
    "    return clist\n",
    "\n",
    "def common_elements(list1, list2):  #paper Alg3 line22\n",
    "    count = 0\n",
    "    for element in list1:\n",
    "        if element in list2:\n",
    "            count= count+1\n",
    "    return count\n",
    "                                            \n",
    "def jaccard_fail(a,b):   #paper Alg2 line14: eliminate duplicates with jaccard similarity\n",
    "    sigma_j = 0.3\n",
    "    inter = common_elements(a,b)\n",
    "    return ((inter/(len(a)+len(b)-inter)) >= sigma_j)\n",
    "    \n",
    "\n",
    "def eliminate_dup2(clist): #paper Alg2 line14: eliminate duplicates with jaccard similarity\n",
    "    buf = []\n",
    "    first = True\n",
    "    for ele in clist:\n",
    "        temp = []\n",
    "        similar = False\n",
    "        if(first):\n",
    "            buf.append(ele)\n",
    "            first = False\n",
    "        else:\n",
    "            for seen in buf:\n",
    "                if jaccard_fail(seen[0],ele[0]):\n",
    "                    similar = True\n",
    "                    if ele[1]>seen[1]:                           \n",
    "                        temp.append(seen)\n",
    "            if len(temp) != 0:  \n",
    "                for seen in temp:\n",
    "                    buf.remove(seen)\n",
    "                buf.append(ele)\n",
    "            elif similar == False:\n",
    "                buf.append(ele)  \n",
    "    return buf\n",
    "\n",
    "\n",
    "\n",
    "def find_avg_score(clist):    #paper Alg3 line23\n",
    "    if len(clist)==0:\n",
    "        return 0\n",
    "    sum = 0\n",
    "    for ele in clist:\n",
    "        sum = sum + ele[1]\n",
    "    return sum/len(clist)\n",
    "\n",
    "def stitch(Canchor,CC):    #paper def 7,Alg3 line23\n",
    "    if len(CC) == 0:\n",
    "        return Canchor\n",
    "    winner = CC[0]\n",
    "    for ele in CC:\n",
    "        if ele[1] > winner[1]:\n",
    "            winner = ele\n",
    "    ans = []\n",
    "    return (Canchor+winner[0])\n",
    "\n",
    "def find_sum(C):             #paper Alg2 line16,17\n",
    "    winner = (None,-1)\n",
    "    runner_up = (None,-1)\n",
    "    for ele in C:\n",
    "        if ele[1] > winner[1]:\n",
    "            if winner[1] > runner_up[1]:\n",
    "                runner_up = winner\n",
    "            winner = ele\n",
    "        elif ele[1] > runner_up[1]:\n",
    "            runner_up = ele\n",
    "    ans1 = \"\"\n",
    "    for words in winner[0]:\n",
    "        ans1 = ans1 + words[0] + \" \"\n",
    "    ans1 = ans1[0].upper() + ans1[1:(len(ans1)-1)]\n",
    "    ans2 = \"\"\n",
    "    for words in runner_up[0]:\n",
    "        ans2 = ans2 + words[0] + \" \"\n",
    "    ans2 = ans2[0].upper() + ans2[1:(len(ans2)-1)]\n",
    "    return ans1,ans2\n",
    "\n",
    "def Traverse(clist,vk,score,PRI_overlap,sent,length):    #paper Alg3\n",
    "    global G1\n",
    "    global PRI1\n",
    "    sigma_r = 2\n",
    "    redundancy = len(PRI_overlap)\n",
    "    if length>10:        #we add a path length limit here, so the program won't have recursion error\n",
    "        return\n",
    "    if (redundancy / length >= sigma_r):  \n",
    "        if (VEN(vk)):\n",
    "            if (valid_sent(sent)):\n",
    "                finalscore = score/length\n",
    "                clist.append((sent,finalscore))\n",
    "                return \n",
    "        for vn in G1[vk]:\n",
    "            PRI_new = PRI_overlap+check_inter(PRI_overlap,PRI1[vn])\n",
    "            redundancy = len(PRI_new)\n",
    "            new_sent = sent+[vn]\n",
    "            L = length + 1\n",
    "            new_score = score + redundancy  \n",
    "            if (collapsible(vn)):\n",
    "                Canchor = new_sent\n",
    "                tmp = []\n",
    "                for vx in G1[vn]:\n",
    "                    Traverse(tmp,vx,0,PRI_new,[vx],L)\n",
    "                    CC = eliminate_dup(tmp) \n",
    "                    avg_CC_score = find_avg_score(CC)\n",
    "                    finalscore = new_score + avg_CC_score\n",
    "                    stitch_sent = stitch(Canchor,CC)  \n",
    "                    if(valid_sent(stitch_sent)):\n",
    "                        clist.append((stitch_sent,finalscore))\n",
    "            else:\n",
    "                Traverse(clist,vn,new_score,PRI_new,new_sent,L)\n",
    "\n",
    "def first_lower(s):     #string processing\n",
    "    if s[0] == \"I\":\n",
    "        return s\n",
    "    else:\n",
    "        return s[0].lower() + s[1:]\n",
    "        \n",
    "def read_text(file_name):    #readfile\n",
    "    redundant = '<br />'\n",
    "    review = []\n",
    "    with open(file_name,'r') as file:\n",
    "        for article in file:\n",
    "            article = article.replace(redundant,\" \")\n",
    "            sentences = nltk.sent_tokenize(article)    \n",
    "            for sentence in sentences:\n",
    "                sentence = first_lower(sentence)\n",
    "                review.append(nltk.pos_tag(nltk.word_tokenize(sentence),tagset='universal'))           \n",
    "    file.close()\n",
    "    return review\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "00_9.txt\n",
      "summary1:  The pettiness of the episode in which a student repeatedly tried\n",
      "summary2:  In the episode in which a student repeatedly tried to burn\n",
      "01_7.txt\n",
      "summary1:  The teachers of the small adventures of the cast is\n",
      "summary2:  The cast is nearly a similar format about the small stories going\n",
      "02_9.txt\n",
      "summary1:  The escapades of a superbly caricatured\n",
      "summary2:  The show does n't shy\n",
      "03_10.txt\n",
      "summary1:  Have to the story to you have to you have to you have\n",
      "summary2:  The story to the world 's a grand experiment that said\n",
      "04_10.txt\n",
      "summary1:  Is in top form in the famous in-the-dark scene .\n",
      "summary2:  In top form in the target of evil guys and\n",
      "05_10.txt\n",
      "summary1:  In the middle of the score of the positive side regarding\n",
      "summary2:  Of the middle of the score of the positive side regarding\n",
      "06_7.txt\n",
      "summary1:  The best part of this movie is the other comments focus\n",
      "summary2:  N't understand why the other comments focus on McConaughey .\n",
      "07_7.txt\n",
      "summary1:  N't like Stanley & Iris tremendously as a film ,\n",
      "summary2:  As a film as a great cinematic entertainment ,\n",
      "08_7.txt\n",
      "summary1:  To have\n",
      "summary2:  For\n",
      "09_9.txt\n",
      "summary1:  This the best at this the elements that make this ,\n",
      "summary2:  This the best at this stand out is the elements that make\n",
      "10_9.txt\n",
      "summary1:  A well done special story of a fine job ,\n",
      "summary2:  Of a story of a fine job ,\n",
      "11_9.txt\n",
      "summary1:  .\n",
      "summary2:  And\n",
      "12_9 .txt\n",
      "summary1:  Was that I was that I was that I was that I saw it was\n",
      "summary2:  A fan of the book ever since third grade ,\n",
      "13_7.txt\n",
      "summary1:  A little help from Jane 's character was just a genre has\n",
      "summary2:  A couple of screens at Yale instead of ordinary people is\n",
      "14_10.txt\n",
      "summary1:  By a great turn by a haunting musical score makes for\n",
      "summary2:  I caught it the first time ,\n",
      "15_8.txt\n",
      "summary1:  The whole thing constantly on the point of barely being\n",
      "summary2:  Fabulous ladies in the whole thing constantly on the film is\n",
      "16_9.txt\n",
      "summary1:  This the best at this the elements that make this ,\n",
      "summary2:  This the best at this stand out is the elements that make\n",
      "17_7.txt\n",
      "summary1:  At the killer 's motive at least the viewer still gets\n",
      "summary2:  A version trimmed by 15 minutes of plot under the film starts\n",
      "18_7.txt\n",
      "summary1:  To a good job in her to read at her to return\n",
      "summary2:  Read at her to read at her to read at her to read\n",
      "19_10.txt\n",
      "summary1:  Is that it is that it is that it is that it is\n",
      "summary2:  Have plenty of reasons to fail him in each the film is\n",
      "20_9.txt\n",
      "summary1:  The beauty of the triumph of the dance that the supporting\n",
      "summary2:  Is the beauty of the triumph of the movie is the supporting\n",
      "21_7.txt\n",
      "summary1:  Is a touching romance with an important theme that stresses the ending\n",
      "summary2:  Is a touching romance with a woman with a job lacking\n",
      "22_8.txt\n",
      "summary1:  The king in the special effects are typical of the acting terribly fake\n",
      "summary2:  Are typical of the king in the special effects are not find\n",
      "23_7.txt\n",
      "summary1:  The ogre bit the dust HARD at the end by having\n",
      "summary2:  The dust HARD at the efforts of Puss in fact has\n",
      "24_8.txt\n",
      "summary1:  The songs in this movie are\n",
      "summary2:  A great children 's story and\n",
      "25_7.txt\n",
      "summary1:  Of the implausibility of the idea of the story is\n",
      "summary2:  In the ogre scene in the idea of the beginning was\n",
      "26_7.txt\n",
      "summary1:  To life 16 years later as he foretold .\n",
      "summary2:  The color is not the film was the appearance that several frames were\n",
      "27_9.txt\n",
      "summary1:  With the threesome 's first shorts with the Stooges as are\n",
      "summary2:  The threesome 's first shorts with the Stooges as are\n",
      "28_9.txt\n",
      "summary1:  The main character of the effect of this film without repeating\n",
      "summary2:  The effect of the main character of this film without repeating\n",
      "29_8.txt\n",
      "summary1:  The whole thing constantly on the point of barely being\n",
      "summary2:  Fabulous ladies in the whole thing constantly on the film is\n",
      "30_10.txt\n",
      "summary1:  Of the departed members of the summer of the show appeared\n",
      "summary2:  I can not wait to buy\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import os\n",
    "filenames = [f for f in os.listdir('.') if os.path.isfile(f)]\n",
    "for name in filenames:         #read all files in the folder\n",
    "    a,b = name.split('.')\n",
    "    if b == 'txt':\n",
    "        print(name)\n",
    "        Z1 = read_text(name)\n",
    "        G1 = dict()\n",
    "        PRI1 = dict()\n",
    "        OpinosisGraph(Z1, G1, PRI1)   #paper Alg2\n",
    "        candidates = []\n",
    "        for keys in G1:\n",
    "            if(VSN(keys)):\n",
    "                clist = []\n",
    "                sum_sent = [keys]\n",
    "                score = 0\n",
    "                Traverse(clist,keys,score,PRI1[keys],sum_sent,1)\n",
    "                candidates = candidates + clist\n",
    "        C = eliminate_dup2(candidates)\n",
    "        summary = find_sum(C)\n",
    "        print(\"summary1: \",summary[0])\n",
    "        print(\"summary2: \",summary[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "problem:\n",
    "1.We incorrectly calculated new PRI by obtaining the union of the two sets, but the paper's method is finding the intersection of two sets\n",
    "2.Performance for some cases is terrible(only a meaningless word is found:08_7.txt,11_9.txt) \n",
    "3.Repetitive rate of two summaries is high for some cases(05_10.txt,16_9.txt)\n",
    "4.The grammar of each sentence is problematic because we don't know how to define a valid sentence\n",
    "'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
