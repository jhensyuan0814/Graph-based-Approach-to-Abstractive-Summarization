{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk                    \n",
    "from operator import eq \n",
    "import math\n",
    "\n",
    "def OpinosisGraph(Z, G, PRI):  \n",
    "    for i in range(len(Z)):  \n",
    "        for j in range(len(Z[i])):  \n",
    "            LABEL = Z[i][j]\n",
    "            PID = j\n",
    "            SID = i\n",
    "            if LABEL in G:\n",
    "                PRI[LABEL].append((SID, PID))\n",
    "            else:\n",
    "                G[LABEL] = list()\n",
    "                PRI[LABEL] = [(SID, PID)]\n",
    "            if j > 0 and LABEL not in G[Z[i][j-1]]:\n",
    "                G[Z[i][j-1]].append(Z[i][j])\n",
    "                \n",
    "def VSN(node):   \n",
    "    global PRI1\n",
    "    sigma_vsn = 15\n",
    "    ele = PRI1[node]\n",
    "    sum = 0\n",
    "    for i in range(len(ele)):\n",
    "        sum = sum + ele[i][1]\n",
    "    return (sum/len(ele)) <= sigma_vsn\n",
    "\n",
    "def VEN(node):  \n",
    "    prohibit = ['and', 'but', 'for', 'nor', 'or', 'so', 'yet', ',', '.', '!', '?']\n",
    "    if(node[0] in prohibit):\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "def collapsible(node): \n",
    "    if(node[1] == 'VERB'):\n",
    "        return True\n",
    "    else: \n",
    "        return False\n",
    "\n",
    "def valid_sent(sent):                        \n",
    "    global no_winner\n",
    "    \n",
    "    all = ['VERB','.','ADJ','PRON','NOUN','ADV','PRON','CONJ','PRT','ADP','NUM','DET','X']\n",
    "    count = dict()\n",
    "    for i in sent:\n",
    "        if i not in count:\n",
    "            count[i] = 1\n",
    "        else:\n",
    "            count[i] += 1\n",
    "    for i in count:\n",
    "        if i[1] in ['ADJ','ADV','NOUN', 'PRON'] and count[i] > 1:\n",
    "            return False\n",
    "    if no_winner == 0:\n",
    "        return True\n",
    "    #the first kind of valid sentence\n",
    "    state = 0\n",
    "    for i in sent:\n",
    "        if state == 0 and i[1] == 'NOUN':\n",
    "            state = 1\n",
    "        if state == 1 and i[1] == 'VERB':\n",
    "            state = 2\n",
    "        if state == 2 and i[1] == 'ADJ':\n",
    "            state = 3\n",
    "    if state == 3:\n",
    "        return True\n",
    "    #the second kind of valid sentence\n",
    "    state = 0\n",
    "    for i in sent:\n",
    "        if state == 0 and i[1] == 'ADJ':\n",
    "            state = 2\n",
    "        if state == 1 and i[1] == 'DET':  \n",
    "            state = 2\n",
    "        if state == 2 and i[1] == 'VERB':\n",
    "            state = 3\n",
    "    if state == 3:\n",
    "        return True\n",
    "    #the third kind of valid sentence\n",
    "    state = 0\n",
    "    for i in sent:\n",
    "        if state == 0 and i[1] == 'ADV':\n",
    "            state = 1\n",
    "        if state == 1 and i[1] == 'ADJ':  \n",
    "            state = 2\n",
    "        if state == 2 and i[1] == 'NOUN':\n",
    "            state = 3\n",
    "    if state == 3:\n",
    "        return True\n",
    "    #the fourth kind of valid sentence\n",
    "    state = 0\n",
    "    for i in sent:\n",
    "        if state == 0 and i[1] == 'ADV':\n",
    "            state = 2\n",
    "        #if state == 1 and i[1] == 'VERB':  #?\n",
    "            #state = 2\n",
    "        if state == 2 and i[1] == 'NOUN':\n",
    "            state = 3\n",
    "    if state == 3:\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "def valid_sent2(sent):    \n",
    "    prohibit = {'VERB':['VERB'],      \n",
    "        'NOUN':['ADJ','PRON','NOUN'],\n",
    "        'ADJ':['VERB','PRON','ADV'],\n",
    "        'ADV':['ADJ','NOUN','PRON','ADV'],\n",
    "        'PRON':['ADJ','PRON','NOUN'], #I,she\n",
    "        'CONJ':['CONJ','PRT','ADP'], #and\n",
    "        'PRT':['CONJ','PRT','ADP'], # 's\n",
    "        '.':['VERB','.','ADJ','PRON','NOUN','ADV','PRON','CONJ','PRT','ADP','NUM','DET','X'], #punctuation\n",
    "        'ADP':['CONJ','PRT','ADP'], #prep+介詞\n",
    "        'NUM':[],\n",
    "        'DET':['CONJ','PRT','ADP'], #to\n",
    "        'X':[]\n",
    "      }\n",
    "    for i in range(0,len(sent)-1):\n",
    "        if(sent[i+1][1] in prohibit[sent[i][1]]):\n",
    "              return False\n",
    "    buf = []\n",
    "    for i in sent:\n",
    "        if(i not in buf):\n",
    "            buf.append(i)\n",
    "        elif(i[1] == 'ADJ' or i[1] == 'ADV' or i[1] == 'NOUN'):\n",
    "            return False\n",
    "    return True\n",
    "\n",
    "def check_inter(PRIa,PRIb):  \n",
    "    PRIc = []\n",
    "    sigma_gap = 4\n",
    "    for first in PRIa:\n",
    "        for second in PRIb:\n",
    "            if(first[0] == second[0] and abs(first[1]-second[1]) <= sigma_gap):\n",
    "                PRIc.append(second)\n",
    "    return PRIc\n",
    "\n",
    "def common_elements(list1, list2): \n",
    "    count = 0\n",
    "    for element in list1:\n",
    "        if element in list2:\n",
    "            count= count+1\n",
    "    return count\n",
    "                                            \n",
    "def jaccard_fail(a,b):\n",
    "    sigma_j = 0.2\n",
    "    inter = common_elements(a,b)\n",
    "    if ((inter/(len(a)+len(b)-inter)) >= sigma_j):\n",
    "        return True\n",
    "    if abs(inter-len(a)) < 3 or abs(inter-len(b)) < 3:\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "def eliminate_dup(clist): \n",
    "    buf = []\n",
    "    first = True\n",
    "    for ele in clist:\n",
    "        temp = []\n",
    "        similar = False\n",
    "        if(first):\n",
    "            buf.append(ele)\n",
    "            first = False\n",
    "        else:\n",
    "            for seen in buf:\n",
    "                if jaccard_fail(seen[0],ele[0]):\n",
    "                    similar = True\n",
    "                    if ele[1]>seen[1]:                           \n",
    "                        temp.append(seen)\n",
    "            if len(temp) != 0:  \n",
    "                for seen in temp:\n",
    "                    buf.remove(seen)\n",
    "                buf.append(ele)\n",
    "            elif similar == False:\n",
    "                buf.append(ele)  \n",
    "    return buf\n",
    "\n",
    "def find_avg_score(clist):    \n",
    "    if len(clist)==0:\n",
    "        return 0\n",
    "    sum = 0\n",
    "    for ele in clist:\n",
    "        sum = sum + ele[1]\n",
    "    return sum/len(clist)\n",
    "\n",
    "def stitch(Canchor,CC):    \n",
    "    if len(CC) == 0:\n",
    "        return Canchor\n",
    "    winner = CC[0]\n",
    "    for ele in CC:\n",
    "        if ele[1] > winner[1]:\n",
    "            winner = ele\n",
    "    ans = []\n",
    "    return (Canchor+winner[0])\n",
    "\n",
    "def Sort_Tuple(tup):  \n",
    "    lst = len(tup)  \n",
    "    for i in range(0, lst):  \n",
    "          \n",
    "        for j in range(0, lst-i-1):  \n",
    "            if (tup[j][1] < tup[j + 1][1]):  \n",
    "                temp = tup[j]  \n",
    "                tup[j]= tup[j + 1]  \n",
    "                tup[j + 1]= temp  \n",
    "    return tup  \n",
    "    \n",
    "\n",
    "\n",
    "def check_verb(sent):\n",
    "    for word,pos in sent:\n",
    "        if pos == 'VERB':\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "def find_sum(C):             \n",
    "    sortC = Sort_Tuple(C)   \n",
    "    if(len(C)==0):\n",
    "        return \"FAIL\"\n",
    "    winner = C[0]\n",
    "    ans1 = \"\"\n",
    "    for words in winner[0]:\n",
    "        ans1 = ans1 + words[0] + \" \"\n",
    "    ans1 = ans1[0].upper() + ans1[1:(len(ans1)-1)]\n",
    "    return ans1\n",
    "    \n",
    "def Traverse(clist,vk,score,PRI_overlap,sent,length): \n",
    "    global ending\n",
    "    #print(score)\n",
    "    global G1\n",
    "    global PRI1\n",
    "    global sigma_r          \n",
    "    redundancy = len(PRI_overlap)\n",
    "    if score > 40:\n",
    "        ending = 1\n",
    "    if ending == 1:\n",
    "        return\n",
    "    if length>15:        \n",
    "        return\n",
    "    if (redundancy  >= sigma_r):  \n",
    "        if (VEN(vk)):\n",
    "            if (valid_sent(sent)):\n",
    "                finalscore = score/length\n",
    "                clist.append((sent,finalscore))\n",
    "                return \n",
    "        for vn in G1[vk]:\n",
    "            PRI_new = check_inter(PRI_overlap,PRI1[vn])  \n",
    "            redundancy = len(PRI_new)\n",
    "            new_sent = sent+[vn]\n",
    "            L = length + 1\n",
    "            new_score = score + redundancy #+ math.log2(L) \n",
    "            if (collapsible(vn)):\n",
    "                Canchor = new_sent\n",
    "                tmp = []\n",
    "                for vx in G1[vn]:\n",
    "                    Traverse(tmp,vx,0,PRI_new,[vx],L)\n",
    "                    CC = eliminate_dup(tmp) \n",
    "                    avg_CC_score = find_avg_score(CC)\n",
    "                    finalscore = new_score + avg_CC_score\n",
    "                    stitch_sent = stitch(Canchor,CC)  \n",
    "                    if(valid_sent(stitch_sent)):\n",
    "                        clist.append((stitch_sent,finalscore))\n",
    "            else:\n",
    "                Traverse(clist,vn,new_score,PRI_new,new_sent,L)\n",
    "\n",
    "def first_lower(s):    \n",
    "    if s[0] == \"I\":\n",
    "        return s\n",
    "    else:\n",
    "        return s[0].lower() + s[1:]\n",
    "        \n",
    "def read_text(file_name):   \n",
    "    redundant = '<br />'\n",
    "    review = []\n",
    "    with open(file_name,'r') as file:\n",
    "        for article in file:\n",
    "            article = article.replace(redundant,\" \")\n",
    "            sentences = nltk.sent_tokenize(article)    \n",
    "            for sentence in sentences:\n",
    "                sentence = first_lower(sentence)\n",
    "                review.append(nltk.pos_tag(nltk.word_tokenize(sentence),tagset='universal'))           \n",
    "    file.close()\n",
    "    return review\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "negative_word = open('negative-words.txt','r')\n",
    "negative = set()\n",
    "for i in range(4784):\n",
    "    a = negative_word.readline()\n",
    "    negative.add(a[:-2])\n",
    "positive_word = open('positive-words.txt','r')\n",
    "positive = set()\n",
    "for i in range(2006):\n",
    "    a = positive_word.readline()\n",
    "    positive.add(a[:-2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_text2(file_name):   \n",
    "    redundant = '<br />'\n",
    "    review = []\n",
    "    with open(file_name,'r') as file:\n",
    "        for article in file:\n",
    "            article = article.replace(redundant,\" \")\n",
    "            sentences = nltk.sent_tokenize(article)    \n",
    "            for sentence in sentences:\n",
    "                sentence = first_lower(sentence)\n",
    "                sentence = sentence.split(' ')\n",
    "                review.append(sentence)     \n",
    "    file.close()\n",
    "    return review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['00_3.txt', 0, 'wrong prediction']\n",
      "['00_9.txt', -0.1111111111111111, 'corrct prediction']\n",
      "['01_1.txt', -0.18181818181818182, 'wrong prediction']\n",
      "['01_7.txt', 0, 'corrct prediction']\n",
      "['02_1.txt', -0.14285714285714285, 'wrong prediction']\n",
      "['02_9.txt', 0, 'corrct prediction']\n",
      "['03_10.txt', 0, 'corrct prediction']\n",
      "['03_4.txt', 0, 'wrong prediction']\n",
      "['04_10.txt', 0.14285714285714285, 'corrct prediction']\n",
      "['04_4.txt', 0, 'wrong prediction']\n",
      "['05_10.txt', 0.1111111111111111, 'corrct prediction']\n",
      "['05_3.txt', 0.06666666666666667, 'wrong prediction']\n",
      "['06_1.txt', 0, 'wrong prediction']\n",
      "['06_7.txt', 0, 'corrct prediction']\n",
      "['07_3.txt', -0.26666666666666666, 'wrong prediction']\n",
      "['07_7.txt', 0, 'corrct prediction']\n",
      "['08_4.txt', 0.0, 'wrong prediction']\n",
      "['08_7.txt', 0, 'corrct prediction']\n",
      "['09_1.txt', -0.3333333333333333, 'wrong prediction']\n",
      "['09_9.txt', -0.125, 'corrct prediction']\n",
      "['10_2.txt', 0, 'wrong prediction']\n",
      "['10_9.txt', 0.25, 'corrct prediction']\n",
      "['11_3.txt', -0.16666666666666666, 'wrong prediction']\n",
      "['11_9.txt', 0, 'corrct prediction']\n",
      "['12_1.txt', -0.058823529411764705, 'wrong prediction']\n",
      "['12_9 .txt', -0.1, 'corrct prediction']\n",
      "['13_2.txt', 0, 'wrong prediction']\n",
      "['13_7.txt', 0.18181818181818182, 'corrct prediction']\n",
      "['14_10.txt', 0, 'corrct prediction']\n",
      "['14_2.txt', 0, 'wrong prediction']\n",
      "['15_1.txt', -0.08571428571428572, 'wrong prediction']\n",
      "['15_8.txt', -0.14285714285714285, 'corrct prediction']\n",
      "['16_3.txt', 0.22727272727272727, 'wrong prediction']\n",
      "['16_9.txt', -0.125, 'corrct prediction']\n",
      "['17_3.txt', 0, 'wrong prediction']\n",
      "['17_7.txt', -0.16666666666666666, 'corrct prediction']\n",
      "['18_3.txt', 0.3333333333333333, 'wrong prediction']\n",
      "['18_7.txt', -0.125, 'corrct prediction']\n",
      "['19_10.txt', 0, 'corrct prediction']\n",
      "['19_4.txt', 0, 'wrong prediction']\n",
      "['20_1.txt', 0.2727272727272727, 'wrong prediction']\n",
      "['20_9.txt', 0.2222222222222222, 'corrct prediction']\n",
      "['21_2.txt', 0.0, 'wrong prediction']\n",
      "['21_7.txt', 0, 'corrct prediction']\n",
      "['22_2.txt', 0.0967741935483871, 'wrong prediction']\n",
      "['22_8.txt', 0, 'corrct prediction']\n",
      "['23_4.txt', 0, 'wrong prediction']\n",
      "['23_7.txt', -0.2857142857142857, 'corrct prediction']\n",
      "['24_4.txt', 0.42857142857142855, 'wrong prediction']\n",
      "['24_8.txt', 0.4, 'corrct prediction']\n",
      "['25_4.txt', 0, 'wrong prediction']\n",
      "['25_7.txt', 0, 'corrct prediction']\n",
      "['26_7.txt', 0, 'corrct prediction']\n",
      "['27_9.txt', 0, 'corrct prediction']\n",
      "['28_9.txt', -0.14285714285714285, 'corrct prediction']\n",
      "['29_8.txt', -0.14285714285714285, 'corrct prediction']\n",
      "['30_10.txt', 0, 'corrct prediction']\n",
      "Prediction Accuracy(based on the text and the summary):  0.543859649122807\n"
     ]
    }
   ],
   "source": [
    "import nltk                                                     #predict based on the text and the summary\n",
    "import os\n",
    "filenames = [f for f in os.listdir('.') if os.path.isfile(f)]\n",
    "filenames.remove('negative-words.txt')\n",
    "filenames.remove('positive-words.txt')\n",
    "new_filenames = []\n",
    "for i in filenames:\n",
    "    if i[-3:] == 'txt':\n",
    "        new_filenames.append(i)\n",
    "result = []\n",
    "for name in new_filenames:   \n",
    "    ending = 0\n",
    "    \n",
    "    a,b = name.split('.')\n",
    "    if b == 'txt':\n",
    "        neutral = 0\n",
    "        sigma_r = 1\n",
    "        no_winner = 0\n",
    "        Z1 = read_text(name)\n",
    "        G1 = dict()\n",
    "        PRI1 = dict()\n",
    "        OpinosisGraph(Z1, G1, PRI1)  \n",
    "        candidates = []\n",
    "        for keys in G1:\n",
    "            if(VSN(keys)):\n",
    "                clist = []\n",
    "                sum_sent = [keys]\n",
    "                score = 0\n",
    "                Traverse(clist,keys,score,PRI1[keys],sum_sent,1)\n",
    "                candidates = candidates + clist\n",
    "        C = eliminate_dup(candidates)\n",
    "        if len(C) == 0:\n",
    "            sigma_r = 1\n",
    "            Z1 = read_text(name)\n",
    "            G1 = dict()\n",
    "            PRI1 = dict()\n",
    "            OpinosisGraph(Z1, G1, PRI1)  \n",
    "            candidates = []\n",
    "            for keys in G1:\n",
    "                if(VSN(keys)):\n",
    "                    clist = []\n",
    "                    sum_sent = [keys]\n",
    "                    score = 0\n",
    "                    Traverse(clist,keys,score,PRI1[keys],sum_sent,1)\n",
    "                    candidates = candidates + clist\n",
    "            C = eliminate_dup(candidates)\n",
    "            break\n",
    "        summary = find_sum(C)\n",
    "        \n",
    "        #interpolation with original data\n",
    "        for word in summary[0]:\n",
    "            if word in negative:\n",
    "                neutral -= 1\n",
    "            elif word in positive:\n",
    "                neutral += 2\n",
    "        a,b = name.split('.')\n",
    "        if b == 'txt':\n",
    "            Z1 = read_text(name)\n",
    "        for i in Z1:\n",
    "            for word in i:\n",
    "                if word[0] in negative and word[1] in ['ADJ','ADV']:\n",
    "                    neutral -= 1/len(Z1)\n",
    "                elif word[0] in positive and word[1] in ['ADJ','ADV']:\n",
    "                    neutral += 2/len(Z1)\n",
    "        result.append([name, neutral])\n",
    "        \n",
    "correct = 0\n",
    "for i in range(len(result)):\n",
    "    a = result[i][0].split('_')\n",
    "    a = a[1].split('.')\n",
    "    if a[0] in ['0','1','2','3','4']:\n",
    "        result[i].append(\"corrct prediction\" if (result[i][1] <= -3) else \"wrong prediction\")\n",
    "    else: \n",
    "        result[i].append(\"corrct prediction\" if (result[i][1] > -3) else \"wrong prediction\")\n",
    "    if result[i][2] == \"corrct prediction\":\n",
    "        correct += 1\n",
    "    print(result[i])  \n",
    "print(\"Prediction Accuracy(based on the text and the summary): \",correct / len(result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['00_3.txt', -1, 'wrong prediction']\n",
      "['00_9.txt', 11, 'correct prediction']\n",
      "['01_1.txt', -8, 'wrong prediction']\n",
      "['01_7.txt', 4, 'correct prediction']\n",
      "['02_1.txt', -3, 'wrong prediction']\n",
      "['02_9.txt', 0, 'correct prediction']\n",
      "['03_10.txt', 9, 'correct prediction']\n",
      "['03_4.txt', 9, 'wrong prediction']\n",
      "['04_10.txt', 4, 'correct prediction']\n",
      "['04_4.txt', 4, 'wrong prediction']\n",
      "['05_10.txt', 7, 'correct prediction']\n",
      "['05_3.txt', 20, 'wrong prediction']\n",
      "['06_1.txt', 9, 'wrong prediction']\n",
      "['06_7.txt', -4, 'correct prediction']\n",
      "['07_3.txt', -1, 'wrong prediction']\n",
      "['07_7.txt', 8, 'correct prediction']\n",
      "['08_4.txt', 0, 'wrong prediction']\n",
      "['08_7.txt', 12, 'correct prediction']\n",
      "['09_1.txt', -2, 'wrong prediction']\n",
      "['09_9.txt', 9, 'correct prediction']\n",
      "['10_2.txt', 12, 'wrong prediction']\n",
      "['10_9.txt', 2, 'correct prediction']\n",
      "['11_3.txt', -7, 'wrong prediction']\n",
      "['11_9.txt', 3, 'correct prediction']\n",
      "['12_1.txt', 2, 'wrong prediction']\n",
      "['12_9 .txt', 10, 'correct prediction']\n",
      "['13_2.txt', 3, 'wrong prediction']\n",
      "['13_7.txt', 5, 'correct prediction']\n",
      "['14_10.txt', -2, 'correct prediction']\n",
      "['14_2.txt', 11, 'wrong prediction']\n",
      "['15_1.txt', 5, 'wrong prediction']\n",
      "['15_8.txt', 1, 'correct prediction']\n",
      "['16_3.txt', 18, 'wrong prediction']\n",
      "['16_9.txt', 9, 'correct prediction']\n",
      "['17_3.txt', 1, 'wrong prediction']\n",
      "['17_7.txt', 1, 'correct prediction']\n",
      "['18_3.txt', 10, 'wrong prediction']\n",
      "['18_7.txt', 22, 'correct prediction']\n",
      "['19_10.txt', 19, 'correct prediction']\n",
      "['19_4.txt', 9, 'wrong prediction']\n",
      "['20_1.txt', 7, 'wrong prediction']\n",
      "['20_9.txt', 10, 'correct prediction']\n",
      "['21_2.txt', 9, 'wrong prediction']\n",
      "['21_7.txt', 3, 'correct prediction']\n",
      "['22_2.txt', 27, 'wrong prediction']\n",
      "['22_8.txt', 4, 'correct prediction']\n",
      "['23_4.txt', 9, 'wrong prediction']\n",
      "['23_7.txt', -4, 'correct prediction']\n",
      "['24_4.txt', -6, 'wrong prediction']\n",
      "['24_8.txt', 0, 'correct prediction']\n",
      "['25_4.txt', 2, 'wrong prediction']\n",
      "['25_7.txt', 4, 'correct prediction']\n",
      "['26_7.txt', 11, 'correct prediction']\n",
      "['27_9.txt', -3, 'correct prediction']\n",
      "['28_9.txt', 19, 'correct prediction']\n",
      "['29_8.txt', 1, 'correct prediction']\n",
      "['30_10.txt', 5, 'correct prediction']\n",
      "Prediction Accuracy(based on only the text):  0.7438596491228071\n"
     ]
    }
   ],
   "source": [
    "filenames = [f for f in os.listdir('.') if os.path.isfile(f)]   #predict based on only the text\n",
    "filenames.remove('negative-words.txt')\n",
    "filenames.remove('positive-words.txt')\n",
    "new_filenames = []\n",
    "for i in filenames:\n",
    "    if i[-3:] == 'txt':\n",
    "        new_filenames.append(i)\n",
    "result = []\n",
    "#new_filenames = ['30_10.txt']\n",
    "for name in new_filenames:         \n",
    "    a,b = name.split('.')\n",
    "    if b == 'txt':\n",
    "        sigma_r = 2\n",
    "        Z1 = read_text2(name)\n",
    "    neutral = 0\n",
    "    for i in Z1:\n",
    "        for word in i:\n",
    "            if word in negative:\n",
    "                neutral -= 1\n",
    "            elif word in positive:\n",
    "                neutral += 2\n",
    "    result.append([name, neutral])\n",
    "\n",
    "correct = 0\n",
    "for i in range(len(result)):\n",
    "    if result[i][0][3: 5] == '10':\n",
    "        result[i].append(\"correct prediction\" if (result[i][1] > -10) else \"wrong prediction\")\n",
    "    elif result[i][0][3] in ['0','1','2','3','4']:\n",
    "        result[i].append(\"correct prediction\" if (result[i][1] < -10) else \"wrong prediction\")\n",
    "    else: \n",
    "        result[i].append(\"correct prediction\" if (result[i][1] > -10) else \"wrong prediction\")\n",
    "    if result[i][2] == \"correct prediction\" :\n",
    "        correct += 1\n",
    "    print(result[i])\n",
    "print(\"Prediction Accuracy(based on only the text): \",correct / len(result))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
